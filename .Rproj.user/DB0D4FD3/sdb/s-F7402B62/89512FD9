{
    "collab_server" : "",
    "contents" : "# loading packages\nlibrary(twitteR)\nlibrary(ROAuth)\nlibrary(tidyverse)\nlibrary(text2vec)\nlibrary(caret)\nlibrary(glmnet)\nlibrary(ggrepel)\n\n### loading and preprocessing a training set of tweets\n# function for converting some symbols\nconv_fun <- function(x) iconv(x, \"latin1\", \"ASCII\", \"\")\n\n##### loading classified tweets ######\n# source: http://help.sentiment140.com/for-students/\n# 0 - the polarity of the tweet (0 = negative, 4 = positive)\n# 1 - the id of the tweet\n# 2 - the date of the tweet\n# 3 - the query. If there is no query, then this value is NO_QUERY.\n# 4 - the user that tweeted\n# 5 - the text of the tweet\n\ntweets_classified <- read_csv('training.1600000.processed.noemoticon.csv',\n                              col_names = c('sentiment', 'id', 'date', 'query', 'user', 'text')) %>%\n  # converting some symbols\n  dmap_at('text', conv_fun) %>%\n  # replacing class values\n  mutate(sentiment = ifelse(sentiment == 0, 0, 1))\n\n# data splitting on train and test\nset.seed(2340)\ntrainIndex <- createDataPartition(tweets_classified$sentiment, p = 0.8, \n                                  list = FALSE, \n                                  times = 1)\ntweets_train <- tweets_classified[trainIndex, ]\ntweets_test <- tweets_classified[-trainIndex, ]\n\n##### doc2vec #####\n# define preprocessing function and tokenization function\nprep_fun <- tolower\ntok_fun <- word_tokenizer\n\nit_train <- itoken(tweets_train$text, \n                   preprocessor = prep_fun, \n                   tokenizer = tok_fun,\n                   ids = tweets_train$id,\n                   progressbar = TRUE)\nit_test <- itoken(tweets_test$text, \n                  preprocessor = prep_fun, \n                  tokenizer = tok_fun,\n                  ids = tweets_test$id,\n                  progressbar = TRUE)\n\n# creating vocabulary and document-term matrix\nvocab <- create_vocabulary(it_train)\nvectorizer <- vocab_vectorizer(vocab)\ndtm_train <- create_dtm(it_train, vectorizer)\ndtm_test <- create_dtm(it_test, vectorizer)\n# define tf-idf model\ntfidf <- TfIdf$new()\n# fit the model to the train data and transform it with the fitted model\ndtm_train_tfidf <- fit_transform(dtm_train, tfidf)\ndtm_test_tfidf <- fit_transform(dtm_test, tfidf)\n\n# train the model\nt1 <- Sys.time()\nglmnet_classifier <- cv.glmnet(x = dtm_train_tfidf, y = tweets_train[['sentiment']], \n                               family = 'binomial', \n                               # L1 penalty\n                               alpha = 1,\n                               # interested in the area under ROC curve\n                               type.measure = \"auc\",\n                               # 5-fold cross-validation\n                               nfolds = 5,\n                               # high value is less accurate, but has faster training\n                               thresh = 1e-3,\n                               # again lower number of iterations for faster training\n                               maxit = 1e3)\nprint(difftime(Sys.time(), t1, units = 'mins'))\n\nplot(glmnet_classifier)\nprint(paste(\"max AUC =\", round(max(glmnet_classifier$cvm), 4)))\n\npreds <- predict(glmnet_classifier, dtm_test_tfidf, type = 'response')[ ,1]\nauc(as.numeric(tweets_test$sentiment), preds)\n\n# save the model for future using\nsaveRDS(glmnet_classifier, 'glmnet_classifier.RDS')\n#######################################################\n\n### fetching tweets ###\ndownload.file(url = \"http://curl.haxx.se/ca/cacert.pem\",\n              destfile = \"cacert.pem\")\nsetup_twitter_oauth('your_api_key', # api key\n                    'your_api_secret', # api secret\n                    'your_access_token', # access token\n                    'your_access_token_secret' # access token secret\n)\n\ndf_tweets <- twListToDF(searchTwitter('setapp OR #setapp', n = 1000, lang = 'en')) %>%\n  # converting some symbols\n  dmap_at('text', conv_fun)\n\n# preprocessing and tokenization\nit_tweets <- itoken(df_tweets$text,\n                    preprocessor = prep_fun,\n                    tokenizer = tok_fun,\n                    ids = df_tweets$id,\n                    progressbar = TRUE)\n\n# creating vocabulary and document-term matrix\ndtm_tweets <- create_dtm(it_tweets, vectorizer)\n\n# transforming data with tf-idf\ndtm_tweets_tfidf <- fit_transform(dtm_tweets, tfidf)\n\n# loading classification model\nglmnet_classifier <- readRDS('glmnet_classifier.RDS')\n\n# predict probabilities of positiveness\npreds_tweets <- predict(glmnet_classifier, dtm_tweets_tfidf, type = 'response')[ ,1]\n\n# adding rates to initial dataset\ndf_tweets$sentiment <- preds_tweets\n\n#######################################################\n\n# color palette\ncols <- c(\"#ce472e\", \"#f05336\", \"#ffd73e\", \"#eec73a\", \"#4ab04a\")\n\nset.seed(932)\nsamp_ind <- sample(c(1:nrow(df_tweets)), nrow(df_tweets) * 0.1) # 10% for labeling\n\n# plotting\nggplot(df_tweets, aes(x = created, y = sentiment, color = sentiment)) +\n  theme_minimal() +\n  scale_color_gradientn(colors = cols, limits = c(0, 1),\n                        breaks = seq(0, 1, by = 1/4),\n                        labels = c(\"0\", round(1/4*1, 1), round(1/4*2, 1), round(1/4*3, 1), round(1/4*4, 1)),\n                        guide = guide_colourbar(ticks = T, nbin = 50, barheight = .5, label = T, barwidth = 10)) +\n  geom_point(aes(color = sentiment), alpha = 0.8) +\n  geom_hline(yintercept = 0.65, color = \"#4ab04a\", size = 1.5, alpha = 0.6, linetype = \"longdash\") +\n  geom_hline(yintercept = 0.35, color = \"#f05336\", size = 1.5, alpha = 0.6, linetype = \"longdash\") +\n  geom_smooth(size = 1.2, alpha = 0.2) +\n  geom_label_repel(data = df_tweets[samp_ind, ],\n                   aes(label = round(sentiment, 2)),\n                   fontface = 'bold',\n                   size = 2.5,\n                   max.iter = 100) +\n  theme(legend.position = 'bottom',\n        legend.direction = \"horizontal\",\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        plot.title = element_text(size = 20, face = \"bold\", vjust = 2, color = 'black', lineheight = 0.8),\n        axis.title.x = element_text(size = 16),\n        axis.title.y = element_text(size = 16),\n        axis.text.y = element_text(size = 8, face = \"bold\", color = 'black'),\n        axis.text.x = element_text(size = 8, face = \"bold\", color = 'black')) +\n  ggtitle(\"Tweets Sentiment rate (probability of positiveness)\")\n\n\n",
    "created" : 1486850748071.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "241982440",
    "id" : "89512FD9",
    "lastKnownWriteTime" : 1486850900,
    "last_content_update" : 1486850900403,
    "path" : "~/TM/sentimentML.R",
    "project_path" : "sentimentML.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}